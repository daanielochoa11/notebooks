{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gisalgs/notebooks/blob/main/spatial-big-data.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Spatial Big Data\n",
    "\n",
    "Big data has been the buzz word for quite a few years now. The main characteristic of big data is its size or volume. For example, think about how many tweets that have ever been twitted and are still coming at this moment. There is a LOT. Also, think about the velocity of these data sets -- they are happening at any moment. There are also a lot of different kinds, or variety, of data sets happening at high volume and high velocity. For example, in addition to tweets, think about how many transactions are made at any moment at Wal-Mart or online shopping sites. These three V's -- volume, velocity, and variety -- are the main characteristics of [big data](https://en.wikipedia.org/wiki/Big_data#Characteristics), along with many other terms, many of them starting with v (veracity, value, and variability). So in short, there is more than enough data out there!\n",
    "\n",
    "In this tutorial, we are going to explore a few ways of collecting big data. We focus on those data sets that have spatial components and we call them spatial big data. \n",
    "\n",
    "## Data feeds\n",
    "\n",
    "To some extent, data feeds are the simplest form of online data. The idea is to have a fixed online file and the data provider will simply keep overwriting the file with new content. A lot of government agencies use this approach to make their data available to the general public with no strings attached. The example we will use here is the data feeds for global earth quakes provided by the USGS as detailed in the page linked below:\n",
    "\n",
    "https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php\n",
    "\n",
    "More specifically, we will use the all quakes in the past day at the following GeoJSON online file:\n",
    "\n",
    "https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson\n",
    "\n",
    "As the name suggest, this is a GeoJSON file and we can use it as we did with the coastlines data. We can actually plot the earth quakes along with the graticule and coastlines, which will provide a quick way of explore the data.\n",
    "\n",
    "We will first import some necessary Python packages that allow us to request the GeoJSON file at a URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we follow the examples we did before by important something from our own modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gisalgs/geom.git \n",
    "\n",
    "from geom.worldmap import *\n",
    "from geom.plot_worldmap import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we get the quakes (again, similar to the coastlines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1day_quakes = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson'\n",
    "with request.urlopen(url_1day_quakes) as response:\n",
    "    quakes1days = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we want to know about what's inside the GeoJSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes1days.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how may earth quakes in the past day around the world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quakes1days['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes1days['features'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes1days['features'][0]['properties'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a quick map, let get the coastlines and graticule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/gisalgs/data/master/ne_110m_coastline.geojson'\n",
    "raw_points, numgraticule, numline = prep_projection_data(url, _use_lib='URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also pull the features for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes_features = quakes1days['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to make a map. We will show two the quakes below magnitude 4.5 and above in two different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1)\n",
    "\n",
    "plot_world(ax, raw_points, numgraticule, numline, 'lightgrey')\n",
    "\n",
    "\n",
    "quakes_45 = [q for q in quakes_features if q['properties']['mag'] > 4.5 ]\n",
    "points = [ [q['geometry']['coordinates'][0], q['geometry']['coordinates'][1]] for q in quakes_45 ]\n",
    "ax.scatter([p[0] for p in points], [p[1] for p in points], color='red', marker='o', alpha=0.8, zorder=2)\n",
    "\n",
    "# TODO\n",
    "#    Draw small (<=4.5) quakes in blue\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Quality Using AirNow API\n",
    "\n",
    "Some web services provide more data and the air quality data provided by [AirNow](airnowapi.org), a web service by the EPA. Any body who wants to access their data must log in. So it is important for each of us to have a valid account at this website. Please sign up using the following link:\n",
    "\n",
    "https://docs.airnowapi.org/account/request/\n",
    "\n",
    "After signing up, AirNow will provide an API Key that is required to access their data sets. The key will come in an email and is also shown on their web site. \n",
    "\n",
    "**TODO: Sign up an airnow account and obtain their API Key**\n",
    "\n",
    "For example, the following is a URL that will return a JSON file for ozone, PM 2.5, and PM 10 air quality indexes for a geographic area around Columbus (please replace YOUR_API_KEY with a real key):\n",
    "\n",
    "```\n",
    "http://www.airnowapi.org/aq/data/?startDate=2016-04-21T12&endDate=2016-04-21T13&parameters=O3,PM25,PM10&BBOX=-83.368244,39.586371,-82.269611,40.344184&dataType=A&format=application/json&verbose=1&API_KEY=YOUR_API_KEY\n",
    "```\n",
    "\n",
    "API stands for application programming interface. What it means is it provides a device in between us and the data that allows us to control in a programming environment. The \"programming\" part of the AirNow API is not directly relevant to us since they do not provide coding specifically. Most of their services are through the URLs like above. They will return a data file and we just need to get the file and its content. Everything here is online and we do not need to download anything.\n",
    "\n",
    "Here, our strategy is simple: we will write code to construct a string that contains the correct URL. What we see above a [**query string**](https://en.wikipedia.org/wiki/Query_string) that includes the web site followed by a question mark. After the question mark we have the parameters, each is formed by something like parameterName=value. Multiple parameters are separated by the ampersand (&).\n",
    "\n",
    "It should be noted that AirNow uses UTC (Coordinated Universal Time), which is 4 hours ahead of us in the Eastern Timezone. So 1 AM Eastern will be 5 AM UTC. THere are modules in Python that are convenient in processing date and time, but here we will simply set time manually. A time of `2020-10-26T05` will be October 26, 2020 at 1 AM Eastern. The following code constructs the correct URL that is ready to use. (Again, please make sure to replace **YOUR_API_KEY** with a real key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'url': 'https://airnowapi.org/aq/data/',\n",
    "    'startDate': '2020-10-26T05',\n",
    "    'endDate': '2020-10-26T06',\n",
    "    'parameters': 'o3,pm25',\n",
    "    'bbox': '-84.815,38.4,-80.5,42',  # OHIO!\n",
    "    'data_type': 'b',\n",
    "    'format': 'application/json',\n",
    "    'verbose': '1',\n",
    "    'api_key': 'YOUR_API_KEY'\n",
    "}\n",
    "\n",
    "# API request URL\n",
    "request_url = options['url'] \\\n",
    "              + '&startDate=' + options['startDate'] \\\n",
    "              + '&endDate=' + options['endDate'] \\\n",
    "              + '?parameters=' + options['parameters'] \\\n",
    "              + '&bbox=' + options['bbox'] \\\n",
    "              + '&datatype=' + options['data_type'] \\\n",
    "              + '&format=' + options['format'] \\\n",
    "              + '&verbose=' + options['verbose']\\\n",
    "              + '&api_key=' + options['api_key']\n",
    "\n",
    "print(request_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the bounding box that contains Ohio. The following code uses the URL to get the stations in Ohio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Perform the AirNow API data request\n",
    "    with request.urlopen(request_url) as response:\n",
    "        responses = json.loads(response.read())\n",
    "except Exception as e:\n",
    "    print('Unable perform AirNowAPI request. %s' % e)\n",
    "    responses = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different stations will have different parameters (ozone or PM 2.5), here is an example of the first station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our goal is to map all ozone air quality indexes across Ohio. We first get all the stations that have ozone measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#    Get all the items in responses that have the Parameters value of OZONE\n",
    "#    and put them in a list called ozone. (Note Parameters is one of the keys.)\n",
    "#    This can be done in a list comprehension, but it will be fine to use a loop.\n",
    "\n",
    "ozone = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = ozone[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o['Longitude'], o['Latitude'], o['SiteName'], o['AQI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a base map for Ohio. Here is an example and other files can be used too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_url = 'https://raw.githubusercontent.com/gisalgs/data/master/OH_geog.geojson'\n",
    "with request.urlopen(oh_url) as response:\n",
    "    oh_counties = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ohio GeoJSON was converted from the [US Census cartographic boundary files](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html) using QGIS. It appears that QGIS will save all polygons as multipolygons. This turns out to be convenient. TO draw the Ohio map along with EPA stations, we first write a function that draws a simply polygon (meaning there is no holes or multiple parts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_polygon(ax, points, color='lightgrey'):\n",
    "    '''\n",
    "    coordinates = [ [x,y], [x,y], ...]\n",
    "    '''\n",
    "    pts = [[p[0], p[1]] for p in points]\n",
    "    l = plt.Polygon(pts, color=color, fill=False, closed=True) # need to import matplotlib.pyplot as plt\n",
    "    ax.add_line(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1)\n",
    "\n",
    "for f in oh_counties['features']:\n",
    "    geom = f['geometry']['coordinates']\n",
    "    for part in geom:\n",
    "        points = part[0]\n",
    "        plot_simple_polygon(ax, points)\n",
    "\n",
    "for s in ozone:\n",
    "    ax.plot(s['Longitude'], s['Latitude'], color='lightgreen', marker='o')\n",
    "    ax.text(s['Longitude'], s['Latitude'], s['AQI'], color='grey')\n",
    "\n",
    "        \n",
    "ax.axis('equal')                        # x and y one the same scale\n",
    "ax.axes.get_xaxis().set_visible(False)  # don't show axis\n",
    "ax.axes.get_yaxis().set_visible(False)  # don't show axis\n",
    "ax.set_frame_on(False)                  # no frame either\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#    Copy the above code here and then make some changes\n",
    "#    so that the labels of the top 5 stations with the hight indexes will in red\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter data and API\n",
    "\n",
    "Twitter data is huge and we can get a piece of it using their API. This API does provide a some coding function and their Python module is called tweepy and can be installed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler, Stream\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Twitter data requires a lot of authentication, which is handled through a lot of tokens. To be able to get those tokens, one must sign up a developer's account at https://developer.twitter.com. The purpose here is to provide a quick introduction to the API and some basic ways to explore the data. The following tokens are already generated by Twitter and are provided here for teaching purposes. These tokens will be deactivated at some point. This is from a free license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {\"consumer_key\": 'ZYD8f3hGFY3tdclRWEXoUhxpM',\n",
    "    \"consumer_secret\": 'V4cjXWVstatZMaoMf8hiE10JCCykNRU82PRTLnN34IU9rXPupT',\n",
    "    \"access_token\": '20824990-5LQAVwk666yeGol0i0ou2vEs2sqSbusPTZOpdk5KD',\n",
    "    \"access_token_secret\": 'x6K30ramXKr4Awu7e8S0nosy5K2R0ZsWix6mU6LOkFTGf'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though a free license, Twitter allows some limited data streaming and Twitter functions. We will stream some geo-tagged twittes from the world. Geo-tagged tweets come in different shapes. Some tweets have specific geolocation provided as coordinates. Some don't, but the provide some information about the place where the tweets are sent. Nevertheless, we will stream tweets by providing a bounding box so that all the tweets sampled here must fall into this region. Our region in this specific case is the globe (note the commented numbers are for Central Ohio):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_bbox = [-180, -90, 180, 90] # [-84.50, 39.50, -81.70, 41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way tweepy works is to define a listener that constantly listens to the twitter world and harvest tweets that are allowed to the stream by the license. Below we extend the listener into our own class called `xGeoStdListener`. \n",
    "\n",
    "Twitter will provide tweets as JSON objects. We will see its detailed information later, but right now we need to be able to save all the JSON objects into a file as a list.\n",
    "\n",
    "In the `__init__` method, the user can give us the duration of this streaming process. By default, we will just stream 10 seconds. Also in this method we get the current time (so we can check duration down the road), and open a file called sometweets.json. We will save all the tweets sampled into this file. This file will be a list of JSON objects and we first write the left bracket into the file. Things will pile up in the file once we get more data from Twitter. We set a counter to zero at the beginning. It is not critical to actually count the number of tweets, but we need to know when the tweets come in. We will add a comma before each tweet (again, a JSON obj). We do this for all the tweets except for the first one (the one immediately after the left bracket).\n",
    "\n",
    "Every time a new tweet is sampled, the API automatically calls the `on_data` method and pass the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xGeoStdListener(StreamListener):\n",
    "    def __init__(self, duration=10):\n",
    "        self.duration = duration\n",
    "        self.time0 = time.time()\n",
    "        self.file = open('sometweets.json', 'w')\n",
    "        self.file.write('[')\n",
    "        self.count = 0\n",
    "    def on_data(self, data):\n",
    "        decoded = json.loads(data) # decode the data so it is all text\n",
    "        if time.time()-self.time0 < self.duration:  # less than 10 seconds\n",
    "            if self.count > 0:                      # if it is the first tweet sampled\n",
    "                self.file.write(',\\n')              # add a comma and a new line\n",
    "            json.dump(decoded, self.file)           # save the decoded text into sometweets.json\n",
    "            self.count += 1\n",
    "            return True\n",
    "        else:\n",
    "            self.file.write(']')                    # add the right bracket\n",
    "            self.file.close()                       # make sure the file is closed\n",
    "            return False\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following first create the listener object and the pass a bunch of codes for authentication. Once the line of `stream = Stream(auth, listener)` is called, the API will start an infinite loop to keep listening. The next line makes sure only geo-tagged tweets are sampled here. Running the following code needs some patience because it takes 10 seconds to finish. \n",
    "\n",
    "It is also important to guess the best time or worst time to collect tweets. Normally most tweets are sent during the day (North America has most of the tweets). During the night things are quiet and we should not expect to many tweets. This will be important when the bounding box is very small.\n",
    "\n",
    "Also, it is necessary to know that the number of geo-tagged tweets is [relatively small](https://gwu-libraries.github.io/sfm-ui/posts/2017-04-12-geographic-collecting), only about 2 percent of all the tweets. So we should also not expect we will get a lot of tweets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    listener = xGeoStdListener()\n",
    "    auth = OAuthHandler(tokens['consumer_key'], tokens['consumer_secret'])\n",
    "    auth.set_access_token(tokens['access_token'], tokens['access_token_secret'])\n",
    "    stream = Stream(auth, listener) # this line supports continuous harvesting\n",
    "    stream.filter(locations=region_bbox)\n",
    "except Exception as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to open the file to retrieve the saved data. Please note the file will be saved into the folder where the program is run, so typically there is no need to change the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sometweets.json') as tweets_file:\n",
    "    tweets = json.load(tweets_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet = tweets[0]\n",
    "tweet.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three keys useful in our case. The `text` key is where the actual tweet is stored. The `coordinates` indicate when the user shares the point location of the tweet. If the user decides to be protect more privacy, he/she can choose to show the cit instead of the coordinates. This is less ideal for us because we won't get the actual coordinates. But this is the case and we will find a way to make it up a little. We can use the `place` key where the bounding box of the region (can be a city or anything) can be found and be used to compute a point.\n",
    "\n",
    "First, let's see how many tweets do not have coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tt = [t for t in tweets if t['coordinates']!=None]\n",
    "len(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code get the tweets with `coordinates` being None, and see what's in the `place` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = [t for t in tweets if t['coordinates']==None]\n",
    "tt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we first get all the geo-tagged tweets by making sure we have the `place` key. Those tweets with `coordinates` not being None still have the `place` key. Sometimes things may go wrong and we will have items in the list without any of those. The following code makes sure only the geo-tagged tweets are used for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotagged = [t for t in tweets if 'place' in t.keys()]\n",
    "len(geotagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the following loop, we make sure every tweet has a valid `coordinates` key. When the original tweet has a value of None, we use the average coordinates from the bounding box to assign to the `coordinates` key. Note the value of this key is actually a JSON with keys of `type` and `coordinates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gt in geotagged:\n",
    "    if not 'coordinates' in gt.keys():\n",
    "        continue\n",
    "    if gt['coordinates'] == None: # use bbox\n",
    "        if gt['place'] != None:\n",
    "            coords = gt['place']['bounding_box']['coordinates'][0]\n",
    "            x = sum([p[0] for p in coords])/4\n",
    "            y = sum([p[1] for p in coords])/4\n",
    "            gt['coordinates'] = {'type': 'Point', \n",
    "                  'coordinates': [x, y]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw the map. To make sure all the dots are on top of the figure, we set the `zorder` to 2 for the does. The default value of 1 will make the dots at the same level of other things in map, making them hard to be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1)\n",
    "\n",
    "plot_world(ax, raw_points, numgraticule, numline, 'lightgrey')\n",
    "\n",
    "\n",
    "points = [ [t['coordinates']['coordinates'][0], t['coordinates']['coordinates'][1]] for t in geotagged ]\n",
    "ax.scatter([p[0] for p in points], [p[1] for p in points], color='green', marker='o', s=3, alpha=0.8, zorder=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do with these tweets? A lot! But how about find some happy tweets? We first will get a list of words that indicate happiness and then check if any of the words appear in the `text`. We must note this is not conclusive at all: we first don't have a complete list of happy words, and then there are tweets in different languages other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = ['happy', 'good', 'wonderful', 'nice', 'eat', 'joyous', 'joy', 'pleased', 'cheer']\n",
    "\n",
    "def has_keywords(keywords, text):\n",
    "    for k in keywords:\n",
    "        if k in text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#    Use the has_keywords function to get all the happy tweets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1)\n",
    "\n",
    "plot_world(ax, raw_points, numgraticule, numline, 'lightgrey')\n",
    "\n",
    "\n",
    "points = [ [t['coordinates']['coordinates'][0], t['coordinates']['coordinates'][1]] for t in geotagged ]\n",
    "ax.scatter([p[0] for p in points], [p[1] for p in points], color='grey', marker='o', s=3, alpha=0.8, zorder=2)\n",
    "\n",
    "# TODO\n",
    "#    Draw the happy tweets in red\n",
    "#    make sure these are on top of everything else\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
